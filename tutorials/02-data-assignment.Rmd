---
title: "Data Management and String Handling"
author: Zach del Rosario
date: 2023-04-07
output: github_document
---

# Data Management and String Handling

*Purpose*: This year's DataFest dataset includes a lot of string data. This tutorial is all about working with data, with a particular focus on string data.

**Important Aside**: I'm going to show you *a lot* of tools. **I am not expecting you to memorize everything.** You will have access to this notebook throughout DataFest; if you want to do something like what you see here, you can just *copy and adapt the code* for your own analysis.

## Optional (But Recommended!) Readings

If you have time before DataFest, I highly recommend working through the following reading:

**Working with data**
- *Optional Reading*: [Isolating Data with dplyr](https://rstudio.cloud/learn/primers/2.2) **Note**: In RStudio use `Ctrl + Click` (Mac `Command + Click`) to follow the link.
- *See also*: [Derive Information with dplyr](https://posit.cloud/learn/primers/2.3)

**Finding patterns in strings**
- *Optional Reading*: [RegexOne](https://regexone.com/) **Note**: In RStudio use `Ctrl + Click` (Mac `Command + Click`) to follow the link.
- *Topics*: All lessons in the Interactive Tutorial, Additional Practice Problems are optional. 
- *Note*: The [stringr cheatsheet](https://github.com/rstudio/cheatsheets/raw/master/strings.pdf) is a helpful reference for this exercise!

# (LIVE DEMO): Analysis of Famous Speeches

In the live demo, I'm going to show you an analysis of famous speeches. This will illustrate a variety of ways to analyze text data. Before we can get there, we'll need to learn some fundamentals of working with data.

```{r}
library(tidyverse)
```

## Working with data: Fundamentals

Remember the `mpg` dataset from the previous tutorial?

```{r}
mpg
```

We spent a lot of time visualizing this dataset. Visualization is one of our most useful tools for making sense of data. However, there are some other really important data management tools we should know how to use.

### Selecting Rows: `filter()`

One of the most useful things we can do is use a `filter` to select particular rows in a dataset. For instance, we might want to find all of the cars that have a particular style of transmission:

```{r}
filter(mpg, trans == "manual(m6)")
```

Using string-detecting patterns, we can make this a bit more general. The following code finds all the vehicles that have manual transmission (not just the 6-speed ones):

```{r}
filter(mpg, str_detect(trans, "manual"))
```

Notice that there are far more vehicles in this dataset. That's because there are far more 5-speed vehicles than 6-speed vehicles. We can see this with a simple `count()`:

```{r}
mpg %>% 
  count(trans) %>% 
  arrange(desc(n))
```


### The Pipe

Above, I used a particular operator called a `pipe`. When using the Tidyverse, we can use the symbol `%>%` to move data from the output of one function to the first input of another function. For instance, if we didn't use the pipe, we would have to write:

```{r}
arrange(count(mpg, trans), desc(n))
```

This code is completely unreadable. That's (partly) because the operations happen "from the inside-out." Re-writing the code with the pipe `%>%` makes it much more readable:

```{r}
mpg %>% 
  count(trans) %>% 
  arrange(desc(n))
```

You can think of the pipe `%>%` as the phrase `and then`. With this in mind, the code above can be read as:

> Start with the dataset `mpg` *and then*
>   count the number of rows according to each `trans` *and then*
>   arrange the dataset in descending order of `n`

I highly recommend using the pipe operator. You'll generally have cleaner, more readable code if you do!

### Deriving values: `mutate()`

Sometimes we need to edit our data. This is frequently because the data are not in the right format for analysis. Othertimes, it's because we want to derive other useful quantities from the data.

To derive values from a dataset, we can use the `mutate()` function. For instance, the following code computes the average of the `hwy` and `cty` fuel economies:

```{r}
mpg %>% 
  mutate(avg = (hwy + cty) / 2) %>% 
  # Move the new `avg` column to the front, for visibility
  select(avg, hwy, cty, everything())
```

We can also use a `mutate` to help clean up strings. For instance, we can remove the number of speeds from `trans` to simplify the column. Here, I use a regular expression to remove the parentheticals from every transmission string:

```{r}
mpg %>% 
  mutate(trans = str_remove(trans, "\\(.*\\)"))
```

This will simplify a call to `count()`:

```{r}
mpg %>% 
  mutate(trans = str_remove(trans, "\\(.*\\)")) %>% 
  count(trans)
```

This shows us (clearly) that automatic vehicles are much more common in this dataset.

## The Gettysburg Address

*Context*: The Gettysburg Address is one of the most famous speeches ever given. That's partly because of its historical importance. But it's also a *very good speech*. Here's one opinion on the matter:

> On June 1, 1865, Senator Charles Sumner referred to the most famous speech ever given by President Abraham Lincoln. In his eulogy on the slain president, he called the Gettysburg Address a "monumental act." He said Lincoln was mistaken that "the world will little note, nor long remember what we say here." Rather, the Bostonian remarked, "The world noted at once what he said, and will never cease to remember it. The battle itself was less important than the speech." (Abraham Lincoln Online)

Here's the full text of the Gettysburg Address:

```{r gettysburg-bliss}
# The full text of the Gettysburg Address (Bliss' copy)
s_gettysburg <- "Four score and seven years ago our fathers brought forth on this continent, a new nation, conceived in Liberty, and dedicated to the proposition that all men are created equal.

Now we are engaged in a great civil war, testing whether that nation, or any nation so conceived and so dedicated, can long endure. We are met on a great battle-field of that war. We have come to dedicate a portion of that field, as a final resting place for those who here gave their lives that that nation might live. It is altogether fitting and proper that we should do this.

But, in a larger sense, we can not dedicate -- we can not consecrate -- we can not hallow -- this ground. The brave men, living and dead, who struggled here, have consecrated it, far above our poor power to add or detract. The world will little note, nor long remember what we say here, but it can never forget what they did here. It is for us the living, rather, to be dedicated here to the unfinished work which they who fought here have thus far so nobly advanced. It is rather for us to be here dedicated to the great task remaining before us -- that from these honored dead we take increased devotion to that cause for which they gave the last full measure of devotion -- that we here highly resolve that these dead shall not have died in vain -- that this nation, under God, shall have a new birth of freedom -- and that government of the people, by the people, for the people, shall not perish from the earth."
```

I'm going to analyze the text of the Gettysburg Address, in order to demonstrate some ways to analyze string data.

## Separate into sentences

The `str_split()` function will split a string on a specified character.

```{r}
str_split("Statement 1, Statement 2", ",")
```

This works as expected when "," is the separating character. If we want to separate text into sentences, we'll need to split on periods. However, we'll run into a surprise if we try to split on ".":

```{r}
str_split(s_gettysburg, ".") %>% 
  .[[1]] %>% 
  .[1:10]
```

The "." character has special meaning for string patterns (regular expressions); to look for a *literal* period, we have to *escape* it with `\\`.

```{r}
str_split(s_gettysburg, "\\.")
```

This has (roughly) split the text into sentences. This is now something we can fit in a dataset.

```{r}
df_gettysburg_raw <- 
  tibble(
    # Note: We have to extract the nested data using brackets [[1]]
    sentence = str_split(s_gettysburg, "\\.")[[1]]
  )

df_gettysburg_raw 
```

The address is now organized into sentences, but there's a bunch of weird characters. The `\n` characters are *newlines*, a form of *whitespace*. We can remove all the whitespace using `str_trim()`.

```{r}
df_gettysburg <- 
  df_gettysburg_raw %>% 
  mutate(sentence = str_trim(sentence))

df_gettysburg 
```

Now that the Gettysburg Address is in a dataset, we can use a variety of data management tools!

## Looking for "but"s

We can search for specific words in a string using `str_detect()`.

```{r}
df_gettysburg %>% 
  mutate(
    but = str_detect(sentence, "but"),
    sentence = str_sub(sentence, end = 20) %>% 
      str_c(., "....")
  ) %>% 
  select(but, sentence)
```

This isn't quite right! Take a closer look at the line that starts `But, in a larger sen....`; that detection failed! We need to catch both upper and lowercase letters. We can use a regular expression to search for both lower and upper-case `b`:

```{r}
df_gettysburg %>% 
  mutate(
    but = str_detect(sentence, "[Bb]ut"),
    sentence = str_sub(sentence, end = 20) %>% 
      str_c(., "....")
  ) %>% 
  select(but, sentence)
```

That's more like it!

### Lincoln's Narrative Index

Now for something surprising! Randy Olson claims that the essence of effective communication can be understood in terms of storytelling. He claims that stories can be understood in terms of three fundamental words: "And", "But", and "Therefore". ABT is a story for another time; for now, we'll focus on the *Narrative Index*, a quantitative tool for assessing how much "story content" there is in a transcript.

*Aside*: For more on the [Narrative Index](http://www.scienceneedsstory.com/blog/the-narrative-index/), check out Olson's website.

The Narrative Index has a very simple definition:

$$NI = \frac{\text{# of `but's in text}}{\text{# of `and's in text}} \times 100$$

Olson claims that a higher NI reflects more "narrative content," and tends to result in more memorable communication. Let's test this assertion! We can start by computing the NI for the Gettysburg Address.

The following code computes the Narrative Index for the Gettysburg address. I use the `str_count()` function to count the occurrences in each sentence, then `summarize()` to sum the counts over every sentence.

```{r}
df_ni_gettysburg <- 
  df_gettysburg %>% 
  mutate(
    n_but = str_count(sentence, "[Bb]ut"),
    n_and = str_count(sentence, "[Aa]nd"),
  ) %>% 
  summarize(
    n_but = sum(n_but),
    n_and = sum(n_and),
  ) %>% 
  mutate(ni = n_but / n_and * 100)
df_ni_gettysburg 
```

The NI is ~33 for the Gettysburg Address. But what does that *mean*? For some context, let's take a look at the NI for a few other transcripts.

## The Lincoln-Douglas Debates

I pre-processed all of the text from the Lincoln-Douglas debates. I did this in a [separate notebook](https://github.com/zdelrosario/datafest2023-tutorials/blob/main/source/02-data-sm.Rmd); the code is a bit involved, but you might find some of the steps there useful for DataFest.

The following code loads the pre-processed Lincoln-Douglas debate transcripts:

```{r}
df_debates <- read_rds("./data/lincoln-douglas-debates.rds")
df_debates
```

Next, I perform the same Narrative Index calculation for each speaker and debate; this is what calling `group_by()` before a `summarize()` accomplishes:

```{r}
df_ni_debates <- 
  df_debates %>% 
  mutate(
    n_but = str_count(sentence, "[Bb]ut"),
    n_and = str_count(sentence, "[Aa]nd"),
  ) %>% 
  group_by(speaker, debate) %>% 
  summarize(
    n_but = sum(n_but),
    n_and = sum(n_and),
  ) %>% 
  mutate(ni = n_but / n_and * 100)
df_ni_debates 
```

Let's visualize the data:

```{r}
df_ni_debates %>% 
  bind_rows(
    df_ni_gettysburg %>% 
      mutate(
        speaker = "Lincoln",
        debate = "(Gettysburg)"
      )
  ) %>% 
  
  ggplot(aes(debate, ni)) +
  geom_col(aes(fill = speaker), position = "dodge") +
  scale_fill_manual(
    values = c("Douglas" = "blue", "Lincoln" = "red")
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 315)) +
  labs(
    x = "Debate",
    y = "Narrative Index\n(# 'but' / # 'and' x 100)"
  )
```

We can make a number of observations:

- Lincoln generally had a higher Narrative Index than Douglas
  - This suggests that Lincoln was following "storytelling" patterns in his debate speeches to a greater degree than Douglas
- The Gettysburg Address had a *much* higher NI than any of these debates
  - This suggests that, even for a "storytelling-heavy" speaker like Lincoln, the Gettysburg Address was a particularly narrative-heavy speech.
  - Given how memorable this speech is, the results seem to agree with Olson's assertion that a high NI corresponds with a more memorable communication.
  
## Relative frequencies

One last analysis; we can look for interesting trends by comparing the frequency of words in our dataset against the frequency of words as they tend to occur in other sources.

The following downloads a word frequency dataset from [Peter Norvig's website](http://norvig.com/ngrams/). This is from the *Google Web Trillion Word Corpus*, which will reflect more modern speech, rather than the contemporary speech patterns at the time of the Gettysburg Address.

*Note*: This is an example use of external data... but since we're showing this example, you probably can't win the "Best Use of External Data" award by just copying this approach!
  
```{r}
url_count1w <- "http://norvig.com/ngrams/count_1w.txt"
filename_count1w <- "./data/count_1w.txt"

## Download the data locally
curl::curl_download(
  url_count1w,
  destfile = filename_count1w
)

## Loads the downloaded file
df_words_web <- read_delim(
  filename_count1w,
  col_names = c("word", "n")
) %>% 
  mutate(f = n / sum(n))
df_words_web
```

We can produce a similar frequency dataset for the Gettysburg Address:
  
```{r}
df_words_gettysburg <- 
  df_gettysburg %>% 
  mutate(word = str_split(sentence, "[[:punct:]\\s]+")) %>% 
  select(word) %>% 
  unnest_longer(word) %>% 
  mutate(word = str_to_lower(word)) %>% 
  count(word) %>% 
  mutate(f = n / sum(n)) %>% 
  arrange(desc(n))

df_words_gettysburg
```
  
By merging the two datasets on the `word` column, we can compare the frequency from the Gettysburg Address against the modern frequency database.

```{r}
df_compare_words <- 
  df_words_gettysburg %>% 
  select(word, n, f) %>% 
  left_join(
    df_words_web %>% select(word, f),
    by = "word",
    suffix = c("_gettysburg", "_web")
  ) %>% 
  mutate(r = f_gettysburg / f_web)

df_compare_words %>% 
  arrange(desc(r))
```

This gives a sense for which words Lincoln used that are not so common (compared to modern speech). This isn't a great estimate for Lincoln's *general* speech patterns (it's a short speech, and many of these uncommon words are used just once), but it does pick out some "curious" words.

# (EXERCISES)

## Using `select()`

The `select()` function helps you to select particular columns. This is helpful for moving columns around to inspect a dataset. One 

```{r}
mpg %>% 
  select(manufacturer, model, class)
```

One useful trick is to use the `everything()` function with `select()`. This will include all of the not-specified columns. This effectively allows you to move columns around, without dropping anything:

```{r}
mpg %>% 
  select(class, everything())
```

### __q1__ Move the columns

Move the columns so that `model` is the leftmost column, and `trans` is to its right. Make sure to preserve all the other columns.

```{r}
# Complete the following code
df_q1 <- 
  mpg
```

Run the following chunk to check your work.

```{r}
if (!all(names(df_q1)[1:2] == c("model", "trans"))) {
  print("Leftmost columns are not correct")
} else if (!setequal(names(df_q1), names(mpg))) {
  print("Your DataFrame is missing columns; did you forget everything()?")
} else {
  print("Correct!")
}
```


## Using `filter()`

The `filter()` function helps us find rows that match certain conditions. We can use any of a variety of comparisons, such as numerical comparisons `x < y`, or exact equality `x == y`. For instance, the following code filters to just one manufacturer.

```{r}
mpg %>% 
  filter(manufacturer == "audi")
```


### __q2__ Filter for an exact string

Filter the dataset `mpg` to return only the rows where the `class` is `"compact"`.

```{r}
# Complete the following code
df_q2 <- 
  mpg
```

Run the following chunk to check your work.

```{r}
if (any(df_q2$class != "compact")) {
  print("Your filter is not correct.")
} else {
  print("Correct!")
}
```


### String handling tools

There are a variety of useful string-handling tools provided in the `tidyverse` package. Here are just a few:

- `str_detect(column, "pattern")`: Detect the presence of a substring (in this case, "pattern")
- `str_count(column, "pattern")`: Count the number of times a substring appears (in this case, "pattern")
- `str_to_lower(column)`: Convert the entire string to lowercase
- `str_to_upper(column)`: Convert the entire string to uppercase

The [stringr cheatsheet](https://www.google.com/search?client=firefox-b-1-d&q=stringr+cheatshet) lists all of the available string-handling tools in the tidyverse. I recommend downloading it and taking a look at your options!

### __q3__ Detect a substring

Filter the `mpg` dataset to only those rows where `model` contains the string `"2wd"`.

```{r}
# Complete the following code
df_q3 <- 
  mpg
```

Run the following chunk to check your work.

```{r}
if (any(!str_detect(df_q3$model, "2wd"))) {
  print("Your filter is not correct.")
} else {
  print("Correct!")
}
```

## Using `mutate()`

The `mutate()` function allows you to derive new values from old ones. For instance, the following code computes the average of the `cty` and `hwy` fuel economy values.

```{r}
mpg %>% 
  mutate(avg = (cty + hwy) / 2) %>% 
  select(avg, cty, hwy, everything())
```


### __q4__ Make lower case

Use an appropriate string-handling tool to change every `sentence` to lower-case.

```{r qX-task}
# Complete the following code
df_q4 <- 
  df_gettysburg
```

Use the following to check your work.

```{r}
if (
  df_q4 %>% 
    pull(sentence) %>% 
    str_detect(., "[A-Z]") %>% 
    any()
) {
  print("Capital letter detected")
} else {
  print("Correct!")
}
```

### __q5__ Count the `"but"`s

Count the number of `"but"`s in each line of the Gettysburg Address. Make sure to match both `"But"` and `"but"`.

```{r}
df_q5 <- 
  df_gettysburg
```

Use the following to check your work.

```{r}
if (
  !all(
    df_q5 %>% 
      pull(sentence) %>% 
      str_count(., "[Bb]ut") ==
    df_q5 %>% 
      pull(n_but)
  )
) {
  print("Incorrect number of 'but's. Did you remember to account for letter case?")
} else {
  print("Correct!")
}
```


## Some useful recipies

Let's go through some of the string manipulations we saw in the Live Demo in slow-motion.

### Breaking sentences into words

Above, I broke sentences down to their individual words. This is helpful for certain kinds of analyses. For instance, let's focus on the following sentence:

```{r}
df_gettysburg %>% 
  pull(sentence) %>% 
  .[[1]]
```

We can split on all whitespace characters by using the pattern `\\s+`: this is called a *regular expression*.

```{r}
df_gettysburg %>% 
  pull(sentence) %>% 
  .[[1]] %>% 
  str_split(., "\\s+")
```

However, that leaves the commas associated with some words! Instead, we can use a collection of different characters; the regular expression `[:punct:]` will capture any and all punctuation. Putting both symbols in square brackets `[...]` means "use either of these." Chaining all of this together

```{r}
df_gettysburg %>% 
  pull(sentence) %>% 
  .[[1]] %>% 
  str_split(., "[[:punct:]\\s]+")
```

### Reshaping the data

Splitting the sentences into words is a good idea, but there's a bit more work we need to do to make the data useful. Look at what happens when we split the sentences into words:

```{r}
df_gettysburg %>% 
  mutate(words = str_split(sentence, "[[:punct:]\\s]+")) %>% 
  select(words)
```

We can't see the actual words! That's because each row "value" is actually a list of words (technically, a character vector). To deal with this, we can use the function `unnest_longer()` to "unravel" each of the word lists into its own row:

```{r}
df_gettysburg %>% 
  mutate(word = str_split(sentence, "[[:punct:]\\s]+")) %>% 
  select(word) %>% 
  unnest_longer(word)
```

This is a far more useful arrangement of the data. You can learn more about tidying data [here](https://posit.cloud/learn/primers/4).

